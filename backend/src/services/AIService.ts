// src/services/AIService.ts

import OpenAI from 'openai';
import { encoding_for_model, TiktokenModel } from 'tiktoken';

/**
 * åµŒå…¥è¯·æ±‚æ¥å£
 */
export interface EmbeddingRequest {
  text: string;
  model?: string;
}

/**
 * åµŒå…¥å“åº”æ¥å£
 */
export interface EmbeddingResponse {
  vector: number[];
  tokens: number;
  model: string;
}

/**
 * èŠå¤©å®Œæˆè¯·æ±‚æ¥å£
 */
export interface ChatCompletionRequest {
  messages: Array<{
    role: 'system' | 'user' | 'assistant';
    content: string;
  }>;
  model?: string;
  maxTokens?: number;
  temperature?: number;
  stream?: boolean;
}

/**
 * èŠå¤©å®Œæˆå“åº”æ¥å£
 */
export interface ChatCompletionResponse {
  content: string;
  tokens: {
    prompt: number;
    completion: number;
    total: number;
  };
  model: string;
  finishReason: string;
}

/**
 * Tokenä½¿ç”¨ç»Ÿè®¡æ¥å£
 */
export interface TokenUsage {
  input: number;
  output: number;
  total: number;
  estimatedCost: number; // ç¾å…ƒ
}

/**
 * AIæœåŠ¡ç±»
 * å°è£…OpenAI APIè°ƒç”¨ï¼Œæä¾›åµŒå…¥ç”Ÿæˆã€èŠå¤©å®Œæˆã€Tokenè®¡æ•°ç­‰åŠŸèƒ½
 */
export class AIService {
  private openai: OpenAI | null = null;
  private encoder: any = null;
  private readonly defaultEmbeddingModel: string;
  private readonly defaultChatModel: string;
  private readonly maxRetries: number;
  private isInitialized: boolean = false;
  private initializationError: string | null = null;

  constructor() {
    // å»¶è¿Ÿåˆå§‹åŒ–ï¼Œä¸åœ¨æ„é€ å‡½æ•°ä¸­éªŒè¯APIå¯†é’¥
    this.defaultEmbeddingModel = process.env.OPENAI_EMBEDDING_MODEL || 'text-embedding-3-small';
    this.defaultChatModel = process.env.OPENAI_MODEL || 'gpt-4';
    this.maxRetries = 3;

    console.log(`ğŸ¤– AIæœåŠ¡æ„é€ å®Œæˆï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰`);
  }

  /**
   * åˆå§‹åŒ– AI æœåŠ¡
   * åœ¨å®é™…ä½¿ç”¨å‰è°ƒç”¨
   */
  private async initialize(): Promise<void> {
    if (this.isInitialized) {
      return;
    }

    try {
      // éªŒè¯APIå¯†é’¥
      const apiKey = process.env.OPENAI_API_KEY;
      if (!apiKey || apiKey.trim() === '') {
        throw new Error('OPENAI_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®æˆ–ä¸ºç©º');
      }

      // åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
      this.openai = new OpenAI({
        apiKey,
        maxRetries: 3,
        timeout: 60000 // 60ç§’è¶…æ—¶
      });

      // åˆå§‹åŒ–Tokenç¼–ç å™¨
      try {
        this.encoder = encoding_for_model(this.defaultEmbeddingModel as TiktokenModel);
      } catch (error) {
        console.warn('æ— æ³•ä¸ºåµŒå…¥æ¨¡å‹åˆ›å»ºç¼–ç å™¨ï¼Œä½¿ç”¨é»˜è®¤ç¼–ç å™¨');
        this.encoder = encoding_for_model('gpt-4');
      }

      this.isInitialized = true;
      console.log(`ğŸ¤– AIæœåŠ¡åˆå§‹åŒ–å®Œæˆ`);
      console.log(`   åµŒå…¥æ¨¡å‹: ${this.defaultEmbeddingModel}`);
      console.log(`   èŠå¤©æ¨¡å‹: ${this.defaultChatModel}`);
    } catch (error) {
      this.initializationError = error.message;
      console.error('AIæœåŠ¡åˆå§‹åŒ–å¤±è´¥:', error.message);
      throw error;
    }
  }

  /**
   * å¥åº·æ£€æŸ¥
   * @returns æ˜¯å¦æœåŠ¡å¯ç”¨
   */
  public async healthCheck(): Promise<boolean> {
    try {
      // å¦‚æœæœ‰åˆå§‹åŒ–é”™è¯¯ï¼Œè¿”å› false
      if (this.initializationError) {
        console.warn('AIæœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼Œå¥åº·æ£€æŸ¥è¿”å›false:', this.initializationError);
        return false;
      }

      // å°è¯•åˆå§‹åŒ–ï¼ˆå¦‚æœè¿˜æ²¡æœ‰åˆå§‹åŒ–ï¼‰
      if (!this.isInitialized) {
        await this.initialize();
      }

      // æ£€æŸ¥OpenAIå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
      if (!this.openai) {
        return false;
      }

      await this.openai.models.list();
      return true;
    } catch (error) {
      console.error('OpenAIå¥åº·æ£€æŸ¥å¤±è´¥:', error.message);
      return false;
    }
  }

  /**
   * ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡
   * @param request åµŒå…¥è¯·æ±‚
   * @returns åµŒå…¥å‘é‡å’Œå…ƒæ•°æ®
   */
  public async createEmbedding(request: EmbeddingRequest): Promise<EmbeddingResponse> {
    // ç¡®ä¿æœåŠ¡å·²åˆå§‹åŒ–
    if (!this.isInitialized) {
      await this.initialize();
    }

    if (!this.openai) {
      throw new Error('OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–');
    }

    const model = request.model || this.defaultEmbeddingModel;
    
    try {
      // é¢„å¤„ç†æ–‡æœ¬
      const cleanText = this.preprocessText(request.text);
      
      // è®¡ç®—Tokenæ•°é‡
      const tokens = this.countTokens(cleanText);
      
      // æ£€æŸ¥Tokené™åˆ¶
      if (tokens > 8191) { // text-embedding-3-smallçš„æœ€å¤§Tokené™åˆ¶
        throw new Error(`æ–‡æœ¬Tokenæ•°é‡ (${tokens}) è¶…è¿‡æ¨¡å‹é™åˆ¶ (8191)`);
      }

      // è°ƒç”¨OpenAI API
      const response = await this.openai.embeddings.create({
        model,
        input: cleanText,
        encoding_format: 'float'
      });

      if (!response.data || response.data.length === 0) {
        throw new Error('OpenAIè¿”å›çš„åµŒå…¥æ•°æ®ä¸ºç©º');
      }

      return {
        vector: response.data[0].embedding,
        tokens: response.usage?.total_tokens || tokens,
        model
      };
    } catch (error) {
      console.error('ç”ŸæˆåµŒå…¥å‘é‡å¤±è´¥:', error);
      throw new Error(`åµŒå…¥ç”Ÿæˆå¤±è´¥: ${error.message}`);
    }
  }

  /**
   * æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡
   * @param texts æ–‡æœ¬æ•°ç»„
   * @param model ä½¿ç”¨çš„æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
   * @returns åµŒå…¥å‘é‡æ•°ç»„
   */
  public async createBatchEmbeddings(
    texts: string[],
    model?: string
  ): Promise<EmbeddingResponse[]> {
    const embeddingModel = model || this.defaultEmbeddingModel;
    const batchSize = 100; // OpenAIå»ºè®®çš„æ‰¹æ¬¡å¤§å°
    const results: EmbeddingResponse[] = [];

    try {
      // åˆ†æ‰¹å¤„ç†
      for (let i = 0; i < texts.length; i += batchSize) {
        const batch = texts.slice(i, i + batchSize);
        const cleanBatch = batch.map(text => this.preprocessText(text));

        console.log(`ğŸ”„ å¤„ç†åµŒå…¥å‘é‡æ‰¹æ¬¡ ${Math.floor(i / batchSize) + 1}/${Math.ceil(texts.length / batchSize)}`);

        const response = await this.openai.embeddings.create({
          model: embeddingModel,
          input: cleanBatch,
          encoding_format: 'float'
        });

        // å¤„ç†æ‰¹æ¬¡å“åº”
        response.data.forEach((embedding, index) => {
          results.push({
            vector: embedding.embedding,
            tokens: this.countTokens(cleanBatch[index]),
            model: embeddingModel
          });
        });

        // æ·»åŠ å»¶è¿Ÿä»¥é¿å…é€Ÿç‡é™åˆ¶
        if (i + batchSize < texts.length) {
          await this.delay(200); // 200mså»¶è¿Ÿ
        }
      }

      console.log(`âœ… æ‰¹é‡åµŒå…¥å®Œæˆï¼Œå…±å¤„ç† ${results.length} ä¸ªæ–‡æœ¬`);
      return results;
    } catch (error) {
      console.error('æ‰¹é‡åµŒå…¥ç”Ÿæˆå¤±è´¥:', error);
      throw error;
    }
  }

  /**
   * èŠå¤©å®Œæˆï¼ˆé—®ç­”ç”Ÿæˆï¼‰
   * @param request èŠå¤©è¯·æ±‚
   * @returns èŠå¤©å“åº”
   */
  public async createChatCompletion(request: ChatCompletionRequest): Promise<ChatCompletionResponse> {
    // ç¡®ä¿æœåŠ¡å·²åˆå§‹åŒ–
    if (!this.isInitialized) {
      await this.initialize();
    }

    if (!this.openai) {
      throw new Error('OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–');
    }

    const model = request.model || this.defaultChatModel;
    const maxTokens = request.maxTokens || parseInt(process.env.OPENAI_MAX_TOKENS || '2000', 10);
    
    try {
      const response = await this.openai.chat.completions.create({
        model,
        messages: request.messages,
        max_tokens: maxTokens,
        temperature: request.temperature || 0.7,
        stream: false // æš‚ä¸æ”¯æŒæµå¼å“åº”
      });

      const choice = response.choices[0];
      if (!choice || !choice.message) {
        throw new Error('OpenAIè¿”å›çš„èŠå¤©å“åº”ä¸ºç©º');
      }

      return {
        content: choice.message.content || '',
        tokens: {
          prompt: response.usage?.prompt_tokens || 0,
          completion: response.usage?.completion_tokens || 0,
          total: response.usage?.total_tokens || 0
        },
        model,
        finishReason: choice.finish_reason || 'unknown'
      };
    } catch (error) {
      console.error('èŠå¤©å®Œæˆå¤±è´¥:', error);
      throw new Error(`èŠå¤©å®Œæˆå¤±è´¥: ${error.message}`);
    }
  }

  /**
   * æµå¼èŠå¤©å®Œæˆï¼ˆå®æ—¶å“åº”ï¼‰
   * @param request èŠå¤©è¯·æ±‚
   * @returns å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œé€æ­¥è¿”å›å“åº”å†…å®¹
   */
  public async *createStreamingChatCompletion(
    request: ChatCompletionRequest
  ): AsyncGenerator<string, ChatCompletionResponse, unknown> {
    const model = request.model || this.defaultChatModel;
    const maxTokens = request.maxTokens || parseInt(process.env.OPENAI_MAX_TOKENS || '2000', 10);
    
    try {
      const stream = await this.openai.chat.completions.create({
        model,
        messages: request.messages,
        max_tokens: maxTokens,
        temperature: request.temperature || 0.7,
        stream: true
      });

      let fullContent = '';
      let finishReason = 'unknown';

      for await (const chunk of stream) {
        const delta = chunk.choices[0]?.delta;
        if (delta?.content) {
          fullContent += delta.content;
          yield delta.content;
        }
        
        if (chunk.choices[0]?.finish_reason) {
          finishReason = chunk.choices[0].finish_reason;
        }
      }

      return {
        content: fullContent,
        tokens: {
          prompt: this.countTokens(request.messages.map(m => m.content).join('\n')),
          completion: this.countTokens(fullContent),
          total: 0 // æµå¼å“åº”ä¸­æ— æ³•ç›´æ¥è·å–
        },
        model,
        finishReason
      };
    } catch (error) {
      console.error('æµå¼èŠå¤©å®Œæˆå¤±è´¥:', error);
      throw error;
    }
  }

  /**
   * è®¡ç®—æ–‡æœ¬Tokenæ•°é‡
   * @param text è¾“å…¥æ–‡æœ¬
   * @param model æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
   * @returns Tokenæ•°é‡
   */
  public countTokens(text: string, model?: string): number {
    try {
      if (!text || typeof text !== 'string') {
        return 0;
      }

      // å¦‚æœencoderè¿˜æ²¡åˆå§‹åŒ–ï¼Œä½¿ç”¨ç²—ç•¥ä¼°ç®—
      if (!this.encoder) {
        const avgCharsPerToken = /[\u4e00-\u9fff]/.test(text) ? 1.5 : 4;
        return Math.ceil(text.length / avgCharsPerToken);
      }

      // å¦‚æœæŒ‡å®šäº†ä¸åŒçš„æ¨¡å‹ï¼Œå°è¯•ä½¿ç”¨å¯¹åº”çš„ç¼–ç å™¨
      if (model && model !== this.defaultEmbeddingModel) {
        try {
          const modelEncoder = encoding_for_model(model as TiktokenModel);
          const tokens = modelEncoder.encode(text);
          modelEncoder.free();
          return tokens.length;
        } catch {
          // å¦‚æœæ¨¡å‹ç¼–ç å™¨ä¸å¯ç”¨ï¼Œå›é€€åˆ°é»˜è®¤ç¼–ç å™¨
        }
      }

      return this.encoder.encode(text).length;
    } catch (error) {
      console.error('Tokenè®¡æ•°å¤±è´¥:', error);
      // ç²—ç•¥ä¼°ç®—ï¼šè‹±æ–‡çº¦4å­—ç¬¦/tokenï¼Œä¸­æ–‡çº¦1.5å­—ç¬¦/token
      const avgCharsPerToken = /[\u4e00-\u9fff]/.test(text) ? 1.5 : 4;
      return Math.ceil(text.length / avgCharsPerToken);
    }
  }

  /**
   * ä¼°ç®—APIè°ƒç”¨æˆæœ¬
   * @param usage Tokenä½¿ç”¨æƒ…å†µ
   * @param model ä½¿ç”¨çš„æ¨¡å‹
   * @returns ä¼°ç®—æˆæœ¬ï¼ˆç¾å…ƒï¼‰
   */
  public estimateCost(usage: TokenUsage, model?: string): number {
    const modelName = model || this.defaultChatModel;
    
    // OpenAIå®šä»·ï¼ˆæˆªè‡³2024å¹´ï¼Œå¯èƒ½ä¼šå˜åŒ–ï¼‰
    const pricing: Record<string, { input: number; output: number }> = {
      'gpt-4': { input: 0.00003, output: 0.00006 },
      'gpt-4-turbo': { input: 0.00001, output: 0.00003 },
      'gpt-3.5-turbo': { input: 0.0000015, output: 0.000002 },
      'text-embedding-3-small': { input: 0.00000002, output: 0 },
      'text-embedding-3-large': { input: 0.00000013, output: 0 }
    };

    const modelPricing = pricing[modelName] || pricing['gpt-4'];
    return (usage.input * modelPricing.input) + (usage.output * modelPricing.output);
  }

  /**
   * æ–‡æœ¬é¢„å¤„ç†
   * @param text åŸå§‹æ–‡æœ¬
   * @returns æ¸…ç†åçš„æ–‡æœ¬
   */
  private preprocessText(text: string): string {
    if (!text || typeof text !== 'string') {
      return '';
    }

    return text
      .trim()
      .replace(/\s+/g, ' ') // åˆå¹¶å¤šä¸ªç©ºç™½å­—ç¬¦
      .replace(/\n{3,}/g, '\n\n') // é™åˆ¶è¿ç»­æ¢è¡Œ
      .substring(0, 32768); // é™åˆ¶æœ€å¤§é•¿åº¦
  }

  /**
   * å»¶è¿Ÿå‡½æ•°ï¼ˆç”¨äºé€Ÿç‡é™åˆ¶ï¼‰
   * @param ms å»¶è¿Ÿæ¯«ç§’æ•°
   */
  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  /**
   * è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
   * @returns æ¨¡å‹åˆ—è¡¨
   */
  public async getAvailableModels(): Promise<string[]> {
    try {
      // ç¡®ä¿æœåŠ¡å·²åˆå§‹åŒ–
      if (!this.isInitialized) {
        await this.initialize();
      }

      if (!this.openai) {
        throw new Error('OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–');
      }

      const response = await this.openai.models.list();
      return response.data.map(model => model.id).sort();
    } catch (error) {
      console.error('è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥:', error);
      return [];
    }
  }

  /**
   * éªŒè¯APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆ
   * @returns æ˜¯å¦æœ‰æ•ˆ
   */
  public async validateApiKey(): Promise<boolean> {
    try {
      // æ£€æŸ¥ç¯å¢ƒå˜é‡
      const apiKey = process.env.OPENAI_API_KEY;
      if (!apiKey || apiKey.trim() === '') {
        return false;
      }

      // ç¡®ä¿æœåŠ¡å·²åˆå§‹åŒ–
      if (!this.isInitialized) {
        await this.initialize();
      }

      if (!this.openai) {
        return false;
      }

      await this.openai.models.list();
      return true;
    } catch (error) {
      console.error('APIå¯†é’¥éªŒè¯å¤±è´¥:', error);
      return false;
    }
  }

  /**
   * é‡Šæ”¾èµ„æº
   */
  public cleanup(): void {
    if (this.encoder) {
      try {
        this.encoder.free();
      } catch (error) {
        console.warn('é‡Šæ”¾ç¼–ç å™¨èµ„æºå¤±è´¥:', error);
      }
    }
    console.log('ğŸ§¹ AIæœåŠ¡èµ„æºå·²æ¸…ç†');
  }
}

/**
 * åˆ›å»ºAIæœåŠ¡å•ä¾‹å®ä¾‹
 * ä½¿ç”¨å»¶è¿Ÿåˆå§‹åŒ–æ¨¡å¼ï¼Œç¡®ä¿å®‰å…¨åˆ›å»º
 */
let aiServiceInstance: AIService | null = null;

export function getAIService(): AIService {
  if (!aiServiceInstance) {
    aiServiceInstance = new AIService();
  }
  return aiServiceInstance;
}

// å¯¼å‡ºå•ä¾‹å®ä¾‹
export const aiService = getAIService();

// é»˜è®¤å¯¼å‡ºç±»
export default AIService;